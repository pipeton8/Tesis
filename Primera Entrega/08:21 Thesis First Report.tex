\documentclass[english, a4paper,12pt]{article}

%:Preamble
\usepackage[bookmarks, colorlinks=true, allcolors=blue, pagebackref=true, hyperfootnotes=false]{hyperref}
\usepackage[eng]{felipito}
\usepackage[round]{natbib}

\graphicspath{{./Graphics/}}

\textheight = 690pt
\topmargin = -40pt
\textwidth = 500pt
\oddsidemargin = -22pt
\spacing{1.5}

%:	Epigraph settings
\setlength{\epigraphwidth}{0.59\textwidth}

%:Document
\begin{document}

%:	Title
\begin{center} \bf \large
	Goodness-of-fit in economic models ?`How much are we losing? \\ Felipe Del Canto, 21 de Agosto de 2019
\end{center}

%:	General importance of models in science and in economics. The assumptions problem and measuring GoF
Every model in science is by definition a simplified reality. In the bright side, abstracting from the complexity of the real world has allowed society to understand the sometimes subtle mechanisms that rule nature and human behavior. This doesn't mean that a model is useful for every purpose. Evidently, whilst some of them may be very useful to expose certain dynamics of the real world, the approximation may carry errors that harm future predictions. This points directly to the question of which model is the most useful for some given problem. In particular, when the answer is \textit{many} the modeler needs to make a choice based on the results she expects to highlight and the channels to study. This dilemma is by no means alien to the field of economics. When describing an economy, the investigator is faced with several possible assumptions that shape the complexity of the model. Although some of them may be made by feasibility reasons (for example, because a highly detailed model can't be solved or simulated or because data won't be available to calibrate it) there may be others that serve a transparency purpose, that is, they intend to make clear the results without dwelling in the unnecessary details. Consequently, in the process of constructing a model, the researcher may choose to follow the Occam's Razor principle: among tqe models that are consistent with the evidence choose the one that makes the fewest assumptions possible. It is implied by this criterion that the measure of a (correct) model is its complexity. However, as Milton Friedman said, ``The ultime goal of a positive science is the development of a `theory' or, `hypothesis' that yields valid and meaningful (...) predictions about phenomena not yet observed'' and thus ``Its performance is to be judged by the precision, scope, and conformity with experience of the predictions it yields''.\footnote{\cite{FriedmanPositive}.}

%: Related literature that supports relevance
Different strands in the economic literature have studied when predictions of some models are robust to different specifications. In \cite{SuttonMarketStruct}, the author discuss which mechanisms in the context of industrial organization still hold in conditions outside the classical models of, for example, Cournot and Bertrand. A similar motivation can be found in \cite{Morris97}, where they study how sensitive are game theory conclusions to the assumption of common knowledge of payoffs in a game. The interest in robustness in the context of mechanism design can be also found in \cite{Morris2011}. An interesting approach is the one in \cite{Basu97} where the authors try to estimate empirically discrepancies due to ``aggregation effects'' when considering a model with a representative firm and one where heterogeneous effects are considered. Similarly, in \cite{SchoolAggregation} the authors try to reconcile contradictory results in the school literature arguing an important role of aggregation in the magnitude of omitted variable bias, which can in principle invalidate previous estimations. Related to this aggregation studies there is a concern with models that make use of aggregated data and different authors have studied what is called ``aggregation bias'' arguing that these models hide important mechanisms that could explain these differences.\footnote{See for example \cite{Agg1, Agg2, Agg3, Agg4}.}  

%:	Representative agent model problems (critics?)
Back to the economic theory, consider the example of the representative agent model. The assumption that there is only one consumer in the economy is useful and has been key to understand important qualitative results, specially in macroeconomics. Nevertheless, employing this model to predict future realizations of certain key variables such as aggregate demand or marginal propensity to consume (MPC) may be inaccurate if heterogeneity effects are in place. In other words, there is a shadow price in the approximation (which the investigator could be willing to pay or not) if she wishes to use the model for another, more quantitative-driven purpose. This point is made clearly in \cite{CarrollRequiem} in the context of the buffer-stock model: ``Representative-agent models are typically calibrated to match an aggregate wealth-to-income ratio'' but ``the typical householdâ€™s wealth is much smaller than the wealth of such a representative agent (...), this would lead one to expect that the behavior of the median household may not resemble the behavior of a representative agent with a wealth-to-income ratio similar to the aggregate ratio''. The evidence quickly backs up this view: while the annual MPC predicted by the representative agent model is about 0.04, many empirical analysis estimate this parameter to lie between 0.2 and 0.5.\footnote{\cite{CarrollRequiem}.} 

%:	Aggregation in particular
The aforementioned model is a particular case of a common practice in economics: aggregation. The other canonical example of its use is aggregation across goods, where instead of describing the myriad of goods available in an economy they are grouped into one or several categories. Regarding these two implementations, previous theoretical literature focused in one side of the problem: when is it possible to carry out this practice and describe precisely the same economy. In the case of the representative agent, the necessary and sufficient condition is that the indirect utility function of every consumer has the Gorman form.\footnote{\cite{Gorman53}.} When aggregation is applied to goods, the answer has been more elusive but two results arise. First, the Hicks-Leontief (composite commodity) theorem allows aggregation if relative prices are constant in the group of goods that are to be bundled. \footnote{\cite{Leontief36, HicksBook}.} Although a somewhat weaker requirement is proposed by \cite{Lewbel96}: bundling is possible if all group relative prices are independent of price indexes and income. The second answer states that group some goods is possible if preferences between them are ``independent'' of the remaining goods present in the economy.\footnote{See for example \cite{GormanSeparability}.}  
 
%:	The idea
For the two kinds of aggregation, the conditions are highly restrictive and not typically met in econometric or theoretical applications. As mentioned previously, the literature has used them in both constructing models and making econometric estimations and this practice comes at a cost. Thus, understand and quantify possible approximation errors is crucial to determine and measure their goodness-of-fit. In contrast with some of the articles mentioned earlier, in this work I intend to give a theoretical look at how these deviations appear using simple models and try to bound them in terms of available variables in the economy. Next, drawing of the previous results, I will discuss a general methodology about approaching this problem in more general settings. Explicitly, in the first part of this work I will review the difference in predictions from both the representative consumer agent and the composite good models when compared with their disaggregate counterparts. For the first model, I will assume that the economy is composed by a continuum of agents but for some reason (simplicity or transparency) the modeler instead considers a representative agent to describe it. Then, assuming a certain distribution for preferences and income, it is possible to bound the difference in prediction for the aggregate demand. This could also be linked to welfare analysis by comparing the difference in utility under different social welfare functions such as the utilitarian or Rawlsian. For the composite good model the main idea is: given some good aggregation and index functions find the utility function that minimizes the differences in predictions and try to construct an upper bound in the prediction error. In the second part of this work, in the context of the buffer-stock model, I will assume a distribution of income that follows certain stochastic process (following \cite{GabaixIncome} and \cite{OkFieldsIncome}) and use properties of the latter to bound the discrepancies between predictions in the MPC from one period to another. For the last part, the discussion of the methodology must comprise at least: how to identify the approximation error in the model, what a good measure of it should incorporate and how to determine if the error is large enough in the economic sense to consider a reformulation of the model. 

\newpage
%: 	Bibliography
\bibliographystyle{abbrvnat}
\bibliography{references}

\end{document}